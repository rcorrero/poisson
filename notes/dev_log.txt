2021/2/1
TODO:
Write code to:
- Test models at varying probability thresholds
- Test models at varying IoU thresholds
- Generate FP-TP curve and calculate AUC for each model


2021/1/26
- Changed `make_target` in `vessel_detector.py` to accept format generated for hand-made test samples.
- Modified bounding-box filtering methods to work with test sample bbox format.

TEST RESULTS:
=====================25%=IoU=50%=Prob=Threshold====================
vd_baseline:
True positives: 0
False positives: 188
Precision: 0.0000
Number boxes: 1002


vd_some_layers:
True positives: 16
False positives: 69
Precision: 0.1882
Number boxes: 1002


vd_all_layers:
True positives: 48
False positives: 18
Precision: 0.7273
Number boxes: 1002
=====================55%=IoU=50%=Prob=Threshold====================
vd_baseline:
True positives: 0
False positives: 188
Precision: 0.0000
Number boxes: 1002


vd_some_layers:
True positives: 8
False positives: 77
Precision: 0.0941
Number boxes: 1002


vd_all_layers:
True positives: 40
False positives: 26
Precision: 0.6061
Number boxes: 1002
======================================================================


2021/1/12
TODO:
- Create code to test object detection models on test images:
- Write code to load bboxes for test imagery (in bboxes.csv) into format appropriate for `evaluate` in vessel_detector.py.
- Calculate mAP for each of the trained models using `evaluate`. 
- Design and implement pipeline for labeling full size PlanetScene images using model:
- Create code to split full size PlanetScene images into 299x299 tiles, label each tile using a model, and combine the outputs into a single result for the entire image.


2020/12/16
- Added `make_model_from_dict` function to `vessel_detector.py` which allows for the loading of model parameters from a state dict. Using this I can continue training the most promising of the models (with pretrained InceptionV3 backbone).
- Started training final model (standard architecture — InceptionV3 backbone trained on vessel imagery plus Faster R-CNN head with standard Fast R-CNN box predictor) with backprop through all layers of the backbone. Although I doubt this model will perform better than the model trained with backprop on only the last three of the backbone layers, it will serve as a good comparison. If it does perform better, then a new model may be trained with backprop on some number of layers of the backbone greater than three but not including all layers to test for an internal optimum. 

After this model is trained, there will be four trained models, each having trained for 30 epochs using the same optimizer and settings:
- pytorch implementation of Faster R-CNN with pretrained ResNet-50 backbone (control)
- Faster R-CNN + InceptionV3 backbone (pretrained by me) with backprop through 0 backbone layers
- Faster R-CNN + InceptionV3 backbone (pretrained by me) with backprop through 3 backbone layers
- Faster R-CNN + InceptionV3 backbone (pretrained by me) with backprop through ALL backbone layers

The next step is therefore to create a test suite on which to compare the models.


2020/12/14
Completed:
- Create code to train baseline vessel detection model using pretrained backbone to use as control and make sure code is functioning properly.
- Change `num_trainable_backbone_layers` parameter in `vessel_detector.py` to allow for all layers of the backbone to be trainable.

I began training a baseline Faster-RCNN model with pretrained ResNet-50 backbone. If the model functions correctly and shows evidence of learning, and all of the metric calculations appear to be correct, then I will decide whether to begin training a new model using the pretrained backbone. If the baseline is performing much better than the model currently being trained, which only allows for the training of three of the inception backbone's layers, I will begin training a new version of the model in which all of the inception model's layers are trainable. This will serve as the best-case scenario for the model, and such an initialization may be viewed as treating the pretrained backbone weights as an initialization for the backbone only. If this also underperforms the baseline, then I will simply use the baseline as the production vessel detection model, and treat the inception backbone as a pre-filter for the vessel detection algorithm.


2020/12/10
Completed:
- Fix mAP calculation in `vessel_detector.py`.
- Fix anchor generator so that the number of anchors is correct
- Change `vessel_detector.py` code such that a set of layers may be trained.


2020/19/9
TODO:
- Figure out how to load model from state_dict


2020/12/7
- Create unittest for VM
- Reformat repo Ð put all .py files in directories
- Increase max number of predicted boxes in RPN
- Verify all hyperparams
- Begin training model

Note: The learning rate in the Faster RÐCNN paper (https://arxiv.org/pdf/1506.01497.pdf)
is too large for my model, and leads to NaN losses. I chose to train with a learning rate
of 1e-4 instead.


2020/11/27
The next step is to use the vessel classification model as a backbone for a vessel
detection model. I began writing the script `vessel_detector.py` which contains
the code for the model.


2020/11/25
- Using the test dataset containing 520 images, I tested the classification
  model. Model generated 0 false positives and identified 12 of the 54 images 
  containing vessels. Because the test data comes from the same distribution as
  target data, it appears likely that the true positive rate will dominate the 
  false positive rate in use, therefore allowing for the derivation of population-
  level results concerning the target vessel fleets.
- It seems useful to test methods for improving model performance using imagery
  generated from PlanetScene imagery. One potential avenue for development is 
  make a training dataset containing approximately 1000 tiles from PlanetScene
  images and retrain the model on this set, using appropriate hyperparameters to
  account for the small dataset size.


2020/11/24
- I created a test dataset containing image tiles segmented from PlanetScene images 
  of Chimbote. Approximately 10% of the samples contain images, and the dataset poses 
  an extreme challange for the algorithm.
- Updated vessel_classifier_test_suite.py to use test images.
- Model performed extremely well on test imagery: attained a precision score of 1.000 and a recall of 0.267, which is much better than I expected considering the difficulty of the imagery.


2020/11/11
- Model has completed 30 epochs since restart (79 overall)
- Validation loss stabilized at roughly 0.05 to 0.07
- Validation accuracy peaked at approximately 0.984
- Total number of samples seen: 15182640


2020/11/6
- Model has completed 11 epochs since restart (49 total)
- Validation loss slightly decreased over the 11 epochs but seems steady
- Validation accuracy peaked at 0.98 on epoch 7 and hovered around 0.975
  on the following epochs.
- VM restarted after 11th epoch; I will restart training and see how that goes.


2020/11/5
- Model has completed 5 epochs since restart (43 total)
- Because valid loss is decreasing consistently it appears that the model still has
  room for improvement.


2020/11/4
- Model completed 25 epochs since restart with smaller weight decay (38 overall)
- Validation loss continuing to drop
- Validation accuracy near 0.975 (moving average estimate - just a ballpark figure)

Because the virtual machine restarted during the 26th epoch, I restarted training.


2020/11/2
- Model completed 13 epochs since restart with smaller weight decay (27 epochs overall)
- Validation loss continuing to drop; currently around 0.085
- Change in validation loss per epoch continuing to decrease suggesting the model
  has almost converged

I intend to let the model continue to train until validation loss begins to consistently increase, 
or until the model has completed 44 epochs overall (30 on this training
session). The model is estimated to complete 44 epochs on or by 2020/11/5 at the current
pace.


2020/11/1
- Model completed 7 epochs with parameters
    - learning rate = 1e-4
    - beta 1 (Adam param.) = 0.9
    - beta 2 (Adam param.) = 0.999
    - weight decay = 1e-7 # Only change from previous training
- Validation loss is steady around 0.082 but decreasing overall (slowly)
- Accuracy on validation set after 20th iter. = 0.964

Next Steps:
    - Get some idea of a target performance level to determine when to finish training (e.g.):
        - Create baseline model (e.g. SVM) and train — compare performance to trained model 
	- Identify mislabeled samples
        - Change hyperparameters according to best practices recs.
    - Create dataset of classified PlanetScene images
    - Test performance on dataset
    - If the performance isn't acceptable then:
	- Add transformations for training data to make more similar to target data
	- Change hyperparams according to best practices
	- Retrain model with PlanetScene samples — follow best practices for situations
          where a model is fine-tuned on a much smaller dataset than the one on which
	  it was originally trained.
	- Repeat until model performs well enough
    - Think about how to use the classification model with the object detection model. 
      The previous object detection model used a ResNet50 architecture as its "backbone". 
      Presumably we can do the same thing with our classification model.	


2020/10/31
- Vessel classifier is training. 
- Model completed 14 epochs with parameters
    - learning rate = 1e-4
    - beta 1 (Adam param.) = 0.9
    - beta 2 (Adam param.) = 0.999
    - weight decay = 1e-5


2020/10/29
- Changed validation set to include only images blurred using Gaussian filter with
  radius = 2. The PlanetScene images are of a lower spatial res. than almost all of 
  the training samples and therefore the valid. set needs to include more blurred 
  images.
- Changed probability of Gaussian blur being applied to each training sample on ingest
  from 0.1 to 0.5 for the same reason.
- Started training